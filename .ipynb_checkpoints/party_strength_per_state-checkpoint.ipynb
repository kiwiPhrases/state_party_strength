{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Party Strength\n",
    "\n",
    "There seems to be no panel dataset of party strength in each state so here I attempt to scrape results from Wikipedia. It's a start and likely also the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import us\n",
    "\n",
    "data_path = \"C:/Users/SpiffyApple/Documents/USC/RaphaelBostic/policy_diffusion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#defining some preliminary functions: \n",
    "def fetch_website(url):\n",
    "    \"\"\"\n",
    "    To hide that the scraping is being done via Python, I change the user-agent to a Firefox\n",
    "    browser so that the website believes it is a chrome browser accessing them. Hope it works.\n",
    "    \"\"\"\n",
    "    user_agent={'User-agent':'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.18 Safari/537.36'}\n",
    "    r=requests.get(url, headers=user_agent)\n",
    "    try:\n",
    "        #print(\"Accessed and downloaded URL data\")\n",
    "        return(r.content)\n",
    "    except ConnectionError:\n",
    "        print(\"Skipping this url\")\n",
    "        return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##get US states to loop over\n",
    "us_states = [state.name for state in us.STATES if state.name != \"District of Columbia\"]\n",
    "us_states.append(\"Washington, D.C.\") #fix a little wikipedia issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##fwtch wiki data and record ones that don't have it\n",
    "out_dict = {}\n",
    "no_wiki = []\n",
    "missing_text = \"Wikipedia does not have an article with this exact name.\"\n",
    "url_base = \"https://en.wikipedia.org/wiki/Political_party_strength_in_\"\n",
    "wiki_urls = [url_base + re.sub(\" \", \"_\",state) for state in us_states]\n",
    "\n",
    "for state in us_states:\n",
    "    out_dict[state] = bs(fetch_website(url_base + re.sub(\" \", \"_\",state)), 'lxml')\n",
    "    if missing_text in out_dict[state].getText():\n",
    "        no_wiki.append(state)\n",
    "        out_dict[state] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_wiki "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the desired tables\n",
    "This is simply done by looking at which tables on the corresponding Wikipedia page contain the year 2014 since that's the year most other datasets do have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#usually there are multiple tables. Pick out ones that contain 2014\n",
    "data_dict = {}\n",
    "for state in us_states:\n",
    "    found_tables = out_dict[state].findAll(\"table\", class_='wikitable')\n",
    "    for table in found_tables:\n",
    "        if \"2014\" in table.getText():\n",
    "            data_dict[state] = table\n",
    "            \n",
    "#check how many states had a table with '2014' in it\n",
    "print(\"Number of states with wikipedia table for party strength: %d\" %len(data_dict.keys()))         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the tables into Pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## corresponding functiosn:\n",
    "def pre_process_table(table):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "        1. table - a bs4 element that contains the desired table: ie <table> ... </table>\n",
    "    OUTPUT:\n",
    "        a tuple of: \n",
    "            1. rows - a list of table rows ie: list of <tr>...</tr> elements\n",
    "            2. num_rows - number of rows in the table\n",
    "            3. num_cols - number of columns in the table\n",
    "    Options:\n",
    "        include_td_head_count - whether to use only th or th and td to count number of columns (default: False)\n",
    "    \"\"\"\n",
    "    rows = [x for x in table.find_all('tr')]\n",
    "\n",
    "    num_rows = len(rows)\n",
    "    \n",
    "    num_cols = max([len(x.find_all(['th','td'])) for x in rows])\n",
    "\n",
    "    ##lets make col_num counter more complicated to account for colspans in headers. \n",
    "    header_rows_set = [x.find_all(['th', 'td']) for x in rows if len(x.find_all(['th', 'td']))>num_cols/2]\n",
    "    \n",
    "    num_cols_set = []\n",
    "\n",
    "    for header_rows in header_rows_set:\n",
    "        num_cols = 0\n",
    "        for cell in header_rows:\n",
    "            row_span, col_span = get_spans(cell)\n",
    "            num_cols+=len([cell.getText()]*col_span)\n",
    "            \n",
    "        num_cols_set.append(num_cols)\n",
    "    \n",
    "    num_cols = max(num_cols_set)\n",
    "    #print(num_cols)\n",
    "    \n",
    "    return (rows, num_rows, num_cols)\n",
    "\n",
    "\n",
    "def get_spans(cell):\n",
    "        \"\"\"\n",
    "        INPUT:\n",
    "            1. cell - a <td>...</td> or <th>...</th> element that contains a table cell entry\n",
    "        OUTPUT:\n",
    "            1. a tuple with the cell's row and col spans\n",
    "        \"\"\"\n",
    "        if cell.has_attr('rowspan'):\n",
    "            rep_row = int(cell.attrs['rowspan'])\n",
    "        else: # ~cell.has_attr('rowspan'):\n",
    "            rep_row = 1\n",
    "        if cell.has_attr('colspan'):\n",
    "            rep_col = int(cell.attrs['colspan'])\n",
    "        else: # ~cell.has_attr('colspan'):\n",
    "            rep_col = 1 \n",
    "        \n",
    "        return (rep_row, rep_col)\n",
    "\n",
    "def process_rows(rows, num_rows, num_cols):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "        1. rows - a list of table rows ie <tr>...</tr> elements\n",
    "    OUTPUT:\n",
    "        1. data - a Pandas dataframe with the html data in it\n",
    "    \"\"\"\n",
    "    data = pd.DataFrame(np.ones((num_rows, num_cols))*np.nan)\n",
    "    for i, row in enumerate(rows):\n",
    "        try:\n",
    "            col_stat = data.iloc[i,:][data.iloc[i,:].isnull()].index[0]\n",
    "        except IndexError:\n",
    "            print(\"Error at row %d\" %i, row, \"\\nParser may have failed to acquire correct number of columns. Num of columns: %d\" %num_cols)\n",
    "            print(\"Failed to locate starting point in above row. Subsequent rows are likely to be erronously parsed\")\n",
    "            \n",
    "        for j, cell in enumerate(row.find_all(['td', 'th'])):\n",
    "            rep_row, rep_col = get_spans(cell)\n",
    "\n",
    "            #print(\"cols {0} to {1} with rep_col={2}\".format(col_stat, col_stat+rep_col, rep_col))\n",
    "            #print(\"\\trows {0} to {1} with rep_row={2}\".format(i, i+rep_row, rep_row))\n",
    "\n",
    "            #find first non-na col and fill that one\n",
    "            while any(data.iloc[i,col_stat:col_stat+rep_col].notnull()):\n",
    "                col_stat+=1\n",
    "\n",
    "            data.iloc[i:i+rep_row,col_stat:col_stat+rep_col] = cell.getText()\n",
    "            if col_stat<data.shape[1]-1:\n",
    "                col_stat+=rep_col\n",
    "\n",
    "    return data\n",
    "\n",
    "def main(table):\n",
    "    rows, num_rows, num_cols = pre_process_table(table)\n",
    "    df = process_rows(rows, num_rows, num_cols)\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## loop through the ditionary:\n",
    "df_dict_origin = {}\n",
    "\n",
    "for state in us_states:\n",
    "    df_dict_origin[state] = main(data_dict[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# in order not to rescrape Wikipedia everytime I need to reset the data/dict, I pickle it. \n",
    "import pickle\n",
    "#with open(\"/\".join([data_path, \"df_dict_raw.pkl\"]), 'wb') as f:\n",
    "#    pickle.dump(df_dict_origin, f)\n",
    "    \n",
    "with open(\"/\".join([data_path, \"df_dict_raw.pkl\"]), 'rb') as f:\n",
    "    df_dict_origin = pickle.load(f)\n",
    "    \n",
    "df_dict = df_dict_origin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Clean and combine the 51 data tables\n",
    "First, perhaps make each datatable into an indexed frame and then work from there. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up the columns.\n",
    "\n",
    "The columns are located in the last two rows of the tables but working with them in Pandas and its string operations is, for some reason, prohibitive. As a result, it is easier to take out the two rows for each table, place them in a list, clean the column names that way, and then re-insert the processed column names into the actual data frames. This, so far, is the most efficient way of working with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "replacement_tuples =  [(\"^gov$\", \"governor\"),\n",
    "    (\"general house assembly\", 'state legislature'),\n",
    "    (\"general court\", 'state legislature'),\n",
    "    (\"general assembly\", \"state legislature\"), \n",
    "    (\"/constitutional\", \"\"),\n",
    "    (\"mayor\", 'governor'),\n",
    "    (\" assem$\", ' house'),\n",
    "    (\" assembly$\", ' house'),\n",
    "    (\"district \\d$\", \"\"),    \n",
    "    (\"representatives\", \"house\"),\n",
    "    (\"legislative assembly\", \"state legislature\"),\n",
    "    (\"united states\", 'us'),\n",
    "    (\"senate\", \"sen\"),\n",
    "    (\"senator\", 'sen'),\n",
    "    (\"^us |:us| u\\.s\\.\", \" \"),\n",
    "    (\"state \", \" \"),\n",
    "    (\"executive offices?:?\", \"\"),\n",
    "    (\"\\(|\\)|:|\\\\n\", \" \"),\n",
    "    (\"\\.\", \"\"),\n",
    "    (\"\\scollege([\\s\\w]+)\", \"\"),\n",
    "    (\"^us\\s\", \" \"),\n",
    "    (\"\\sus\", \" \"),             \n",
    "    (\"assem$\", 'house'),      \n",
    "    (\"\\siii\", \" 3\"),\n",
    "    (\"\\sii\", \" 2\"),                   \n",
    "    (\"\\si\", \" 1\"),                       \n",
    "    (\"state legislature legislature\", \"state legislature house\"),(\"\\s\\s\", \" \")]\n",
    "\n",
    "def bulk_sub(entry):\n",
    "    for s, re_s in replacement_tuples:\n",
    "        entry = re.sub(s,re_s, entry)\n",
    "        entry = entry.strip()\n",
    "    return(entry)\n",
    "\n",
    "column_dict = {}\n",
    "for state in us_states:\n",
    "    column_dict[state] = list(df_dict[state].iloc[[-1,-2],:].apply(lambda x: x.str.lower(), axis=1).astype(str).apply(lambda x: \":\".join(x)))\n",
    "\n",
    "for state in us_states:\n",
    "    column_dict[state] = [bulk_sub(entry) for entry in column_dict[state]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-column processing\n",
    "Insert the processed column names, fix index, take out unnecessary info, and combine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "join_row = lambda x: \",\".join(x)\n",
    "##it may actually make sense to make columns from the second to last row, then drop duplicates if necessary, set index, etc\n",
    "for state in us_states:\n",
    "    df_dict[state].columns = column_dict[state]\n",
    "    df_dict[state].columns.name = None\n",
    "    \n",
    "    ##eliminate the last two rows:\n",
    "    df_dict[state] = df_dict[state].iloc[2:-3,:]   \n",
    "    \n",
    "    df_dict[state].dropna(axis=1, how='all', inplace=True) \n",
    "    \n",
    "df_dict['Washington, D.C.'].rename(columns = {'congress house delegate':\"congress house\",\n",
    "                                              \"congress shadow sen seat 1\":\"congress sen class 1\",\n",
    "                                             'congress shadow sen seat 2':\"congress sen class 2\",\n",
    "                                             'presidential electoral':\"electoral\"}, inplace=True)\n",
    "df_dict['Washington, D.C.'].drop(\"congress shadow representative\", axis=1, inplace=True)\n",
    "df_dict['Maine'].drop(\"congress former house districts\", axis=1, inplace=True)\n",
    "\n",
    "for state in us_states:\n",
    "    bool_array = df_dict[state].columns.str.contains(\"congress house\")\n",
    "    if np.sum(bool_array)>1:\n",
    "        temp = df_dict[state].loc[:,bool_array].astype(str).apply(join_row,axis=1)\n",
    "        df_dict[state] = df_dict[state].loc[:,~bool_array]\n",
    "        df_dict[state].loc[:,'congress house'] = temp \n",
    "        \n",
    "    df_dict[state] = df_dict[state].loc[:,~df_dict[state].columns.duplicated()]\n",
    "    \n",
    "    #index setting and cleaning\n",
    "    try:\n",
    "        df_dict[state].set_index('year year', inplace=True)\n",
    "    except KeyError:\n",
    "        print(state)\n",
    "        break \n",
    "    df_dict[state] = df_dict[state][df_dict[state].index.str.contains(\"\\d\")]\n",
    "    df_dict[state].index.name = 'year'\n",
    "    \n",
    "    df_dict[state].index = df_dict[state].index.str.extract(\"(\\d+)\", expand=False)\n",
    "    df_dict[state].index = df_dict[state].index.astype(np.int64)\n",
    "    df_dict[state] = df_dict[state].loc[df_dict[state].index>=1980]\n",
    "    df_dict[state].dropna(axis=1, how='all', inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for state in us_states:\n",
    "    print(state+\"\\n\",df_dict[state].loc[:,df_dict[state].columns.str.contains('^governor|house|sen|electoral')].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "combnd = pd.concat([df_dict[state].loc[:,df_dict[state].columns.str.contains('^governor|house|sen|electoral')] for state in us_states], keys=us_states)\n",
    "combnd.index.name = ['state','year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combnd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combnd.to_csv(\"/\".join([data_path, \"states_party_strength.csv\"]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
